#+TITLE: SuperScreecher9000 Roadmap
#+DATE: 2026-01-10
#+DESCRIPTION: Design document for speech-to-cursor accessibility tool

* Overview

A Rust daemon that captures speech via hotkey, transcribes locally via Whisper,
and outputs text at cursor position. Foundation for accessibility tooling.

When combined with an MCP-capable AI (Claude Code, etc.) and rmcp-presence,
this enables full voice-controlled desktop interaction.

* The Pipeline

#+BEGIN_SRC
[User presses hotkey]
       ↓
[Daemon starts recording via cpal]
       ↓
[User releases hotkey]
       ↓
[Audio buffer sent to Whisper]
       ↓
[Whisper returns transcription]
       ↓
[Text typed at cursor position]
       ↓
[If cursor is in AI chat: AI processes, may call MCP tools]
       ↓
[rmcp-presence executes desktop actions]
#+END_SRC

* Components

** 1. Global Hotkey Listener [DONE]
- Crate: ~rdev~ v0.5 - *WORKS FIRST TRY*
- Captures key press/release events globally
- Using F12 as default hotkey
- Cross-platform (X11 tested, Wayland remains a problem)

** 2. Audio Capture [DONE]
- Crate: ~cpal~ v0.15 (cross-platform)
- Capture from default input device
- Buffer audio while hotkey held
- Native format likely 44.1kHz/48kHz stereo, needs conversion

*** Whisper Audio Requirements
- Sample rate: *16kHz* (native, no resampling needed)
- Channels: *Mono* (single channel)
- Format: *Float32* samples (normalized -1.0 to 1.0)
- Can pass ~Vec<f32>~ directly to whisper-rs, no file needed

*** Resampling
- If cpal gives us 44.1kHz/48kHz, need to downsample
- Crates: ~rubato~ or ~dasp~ for resampling
- Or: simple averaging for quick-and-dirty mono conversion

*** Ring Buffer Recording (Debug/History Feature)
- Save recordings to ~/.ss9k/recordings/
- Timestamp-based filenames: 2026-01-10_11-45-23.wav
- Config: max_recordings or max_size_mb
- Delete oldest when limit reached
- Uses ~hound~ crate for WAV writing
- Use cases:
  - Debug transcription errors
  - Recovery if whisper fails
  - History ("what did I just say?")
  - Potential training data

** 3. Whisper Inference [DONE]
- Local inference, no API costs
- Crate: ~whisper-rs~ v0.14 (bindings to whisper.cpp) - *COMPILED FIRST TRY*
- Resampling: ~rubato~ v0.15 for 44.1kHz → 16kHz
- Input: ~Vec<f32>~ at 16kHz mono
- Output: transcription string
- Using base model (~142MB) - works but slow on CPU
- Model stored at: =models/ggml-base.bin=
- GPU acceleration desirable but not required

** 4. Text Output [DONE]
- Type transcription at current cursor position
- Crate: ~enigo~ v0.2 (cross-platform keyboard simulation)
- Already proven in rmcp-presence
- *WORKS!* Types wherever cursor is focused

* MVP Scope

1. Linux only (start with what we have)
2. Single hardcoded hotkey (e.g., Ctrl+Shift+Space)
3. Whisper tiny/base model for speed
4. Type at cursor, no special handling

* The Vision: Cross-Platform Voice-to-System-Control

#+BEGIN_CENTER
*BRINGING MODAL TEXT EDITING TO A STT NEAR YOU*

/vim users rejoice: now you can :wq with your mouth/
#+END_CENTER

The first true cross-platform, single-binary, speech-to-system-control tool.

- Download one binary
- Run it
- Talk to your computer
- It listens and acts

No Python. No runtime. No config wizard. No cloud. Just works.

*Everyone else is doing dictation. We're doing domination.*

** Why This Doesn't Exist Yet

- *Talon*: Framework, requires Python + setup, learning curve
- *Voxtype*: Linux/Wayland focused, dictation only (no system commands)
- *OpenWhispr/Handy*: Dictation only, no system control
- *OpenVoiceOS*: Voice assistant (Alexa-style), not desktop control
- *OS built-ins*: Platform-locked, often cloud-dependent, not customizable

The gap: **dictation + system commands + cross-platform + single binary**

** Architecture

#+BEGIN_SRC rust
trait SystemControl {
    fn focus_window(&self, query: &str) -> Result<()>;
    fn set_volume(&self, level: u8) -> Result<()>;
    fn media_next(&self) -> Result<()>;
    fn media_prev(&self) -> Result<()>;
    fn media_play_pause(&self) -> Result<()>;
    fn launch_app(&self, name: &str) -> Result<()>;
    fn screenshot(&self) -> Result<PathBuf>;
    fn lock_screen(&self) -> Result<()>;
    // etc.
}

#[cfg(target_os = "linux")]
struct LinuxControl;

#[cfg(target_os = "windows")]
struct WindowsControl;

#[cfg(target_os = "macos")]
struct MacControl;
#+END_SRC

Same voice commands everywhere. Platform-specific guts.

** Platform Backends

*** Linux
- Window management: i3-msg, swaymsg, wmctrl (fallbacks)
- Audio: pactl (PulseAudio), wpctl (PipeWire)
- Media: playerctl (MPRIS/D-Bus)
- Apps: xdg-open, direct execution
- Display: X11 (working), Wayland (needs portal API for hotkeys)
- Dependencies: mostly shelling out, some D-Bus via zbus

*** Windows
- Window management: Win32 API (FindWindow, SetForegroundWindow)
- Audio: Windows Audio Session API (WASAPI)
- Media: Windows.Media.Control namespace
- Apps: ShellExecute
- Dependencies: windows-sys crate, possibly COM bindings
- Note: Actually well-documented, might be easier than expected

*** macOS
- Window management: AppleScript, Accessibility API
- Audio: CoreAudio
- Media: MediaPlayer framework, AppleScript to Music.app
- Apps: NSWorkspace, open command
- Dependencies: cocoa, core-foundation, objc2
- Note: Accessibility permissions required, will prompt user
- Note: AppleScript can do a LOT, might be the escape hatch

** Voice Commands (Expanded)

*** Current (v0.4)
| Command    | Action       |
|------------+--------------|
| enter      | Press Enter  |
| tab        | Press Tab    |
| escape     | Press Escape |
| backspace  | Backspace    |
| space      | Press Space  |
| copy       | Ctrl+C       |
| paste      | Ctrl+V       |
| cut        | Ctrl+X       |
| undo       | Ctrl+Z       |
| redo       | Ctrl+Shift+Z |
| save       | Ctrl+S       |
| select all | Ctrl+A       |

*** Planned (System Control)
| Command              | Action                   |
|----------------------+--------------------------|
| volume up/down       | Adjust system volume     |
| mute                 | Toggle mute              |
| next song / previous | Media control            |
| play / pause         | Media play/pause         |
| switch to [app]      | Focus window by name     |
| open [app]           | Launch application       |
| close window         | Close focused window     |
| screenshot           | Capture screen           |
| lock screen          | Lock session             |
| scroll up/down       | Scroll in focused window |
| click                | Click at cursor          |

*** Future (Power User)
| Command               | Action                   |
|-----------------------+--------------------------|
| workspace [n]         | Switch workspace/desktop |
| move to workspace [n] | Move window              |
| fullscreen            | Toggle fullscreen        |
| minimize / maximize   | Window control           |
| sleep / hibernate     | System power             |
| brightness up/down    | Display brightness       |

** Dependency Strategy

Feature flags to minimize bloat:

#+BEGIN_SRC toml
[features]
default = ["linux"]
linux = ["zbus", "x11rb"]
windows = ["windows-sys"]
macos = ["cocoa", "objc2"]

# GPU backends
vulkan = ["whisper-rs/vulkan"]
cuda = ["whisper-rs/cuda"]
metal = ["whisper-rs/metal"]
#+END_SRC

CI builds separate binaries per platform. Users download the right one.

** Distribution Plan

- GitHub Releases with prebuilt binaries
- Linux: AppImage or static binary
- Windows: .exe (maybe .msi installer later)
- macOS: .dmg or unsigned binary (Gatekeeper warning)
- Package managers eventually: AUR, Homebrew, winget

** Testing Matrix (The Nightmare)

| OS         | WM/DE        | Audio      | Status        |
|------------+--------------+------------+---------------|
| Linux      | i3/X11       | PulseAudio | Primary dev   |
| Linux      | Sway/Wayland | PipeWire   | Needs testing |
| Linux      | GNOME        | PipeWire   | Needs testing |
| Linux      | KDE          | PulseAudio | Needs testing |
| Windows 10 | -            | WASAPI     | Needs dev     |
| Windows 11 | -            | WASAPI     | Needs dev     |
| macOS 13+  | -            | CoreAudio  | Needs dev     |

* Future Enhancements

- [X] Cross-platform (Windows, macOS) → IN PLANNING
- [ ] Configurable hotkey
- [ ] Push-to-talk vs toggle mode
- [ ] Audio feedback (beep on start/stop recording)
- [ ] Visual indicator (tray icon, LED if hardware)
- [ ] Hardware trigger support (bluetooth button)
- [ ] Wearable mic/button (ESP32/Pi Pico)
- [ ] Streaming transcription (type as you speak)
- [X] Command mode vs dictation mode → Voice commands implemented!
- [ ] Integration with screen readers

* Open Questions

- [X] Which Whisper binding is most reliable? → whisper-rs v0.14, compiled first try
- [X] What's acceptable latency for transcription? → ~15s on old hardware is usable for accessibility
- [ ] How to handle Wayland (no global hotkey support)? → Still open, maybe portal API?
- [X] GPU inference worth the complexity? → YES, just a feature flag, massive improvement
- [X] Ship model with binary or download on first run? → Ship all models (~5GB), users pick

* Accessibility Impact

This isn't just a convenience tool. For users with:
- Limited mobility
- RSI/carpal tunnel
- Visual impairments (combined with screen reader)
- Temporary injuries

...this provides an alternative input method that, combined with AI + MCP,
enables full desktop control through voice.

* Project Name

*SuperScreecher9000*

Because we're serious professionals who build serious tools.

* Build Log

** 2026-01-10 - Session 1: Project Birth
- Created ss9k project via cargo new
- Added rdev, cpal, enigo, anyhow dependencies
- Implemented basic hotkey listener with F12
- *HOTKEY COMPILED AND WORKED FIRST TRY*
- Global hotkey capture confirmed working on X11
- Audio capture debugging: ALSA default device showed 0 callbacks
- Fixed by searching for device with "Microphone" + "CARD" in name
- Found: =sysdefault:CARD=Microphone= (USB Condenser Microphone)
- *AUDIO CAPTURE WORKING!* 110k+ samples (2.5s), 334 callbacks
- Pipeline: Hotkey ✅ | Audio Capture ✅ | Whisper TODO | Type Output TODO
- Next: whisper-rs integration for transcription

** 2026-01-10 - Session 2: WHISPER WORKS
- Added whisper-rs v0.14 and rubato v0.15 dependencies
- *WHISPER-RS COMPILED FIRST TRY* (whisper.cpp built successfully)
- Downloaded ggml-base.bin model (~142MB) from HuggingFace
- Implemented resample_audio() using rubato SincFixedIn
- Implemented transcribe() with WhisperContext
- Model loads on startup, inference runs on F12 release
- *FULL PIPELINE WORKING!*
- First transcription: "[clicking]" (lol keyboard noise)
- Second transcription: "We snore. Yeah, we snore." (poetry)
- Pipeline: Hotkey ✅ | Audio Capture ✅ | Whisper ✅ | Type Output TODO
- Next: enigo integration to type at cursor

** 2026-01-10 - Session 2b: MVP COMPLETE!
- Added enigo text output
- *FULL PIPELINE WORKING END TO END*
- Test transcriptions typed successfully:
  - "Yeah, I forgot to type."
  - "working so far, so pretty good."
  - "I'm not even dictated clearly and it's picking me up already."
  - "Can I get a yeehaw and/or a yeehaw or a weak snarl?"
- Pipeline: Hotkey ✅ | Audio Capture ✅ | Whisper ✅ | Type Output ✅
- *MVP COMPLETE* - SuperScreecher9000 is born!

** 2026-01-10 - Session 3: GPU ACCELERATION + MODEL TESTING
- Enabled Vulkan GPU backend via whisper-rs feature flag
- *VULKAN WORKING ON INTEL HD 530* - integrated GPU doing inference!
- Downloaded ALL models: tiny (75MB), base (142MB), small (466MB), medium (1.5GB), large-v3 (3GB)
- Total models: ~5GB - acceptable for accessibility tool
- Tested CUDA/Metal features - require platform-specific toolkits at compile time
- Model quality comparison:
  - base: "weesnaw" → "we snore", "typing" → "taping"
  - small: Significantly better quality, ~3x size
  - medium: Excellent quality, ~15s inference on i5-6500
- Real-world testing via actual usage (meta: used SS9K to discuss SS9K)
- Found: "asterisks" → "ass risks" (lmao)
- Found: Caps lock interferes with hotkey capture
- Resource usage: Low CPU/GPU idle, reasonable during inference
- Decision: Ship all models (~5GB total), let users pick
- Created README.md with full documentation
- Pipeline: Hotkey ✅ | Audio ✅ | Whisper ✅ | Type ✅ | GPU ✅

** 2026-01-10 - Session 4: VISION CRYSTALLIZED
- Added config file support (toml + serde)
- Added auto-download from HuggingFace with progress bar (reqwest + indicatif)
- Added voice command parsing with punctuation stripping
- Commands: enter, tab, escape, backspace, copy, paste, cut, undo, redo, save, select all
- Fixed: Whisper outputs "Enter." but we need to match "enter"
- Binary size: 2MB → 13MB (HTTP/TLS deps, worth it)
- Researched competition: Voxtype, Talon, OpenWhispr, Handy
- Key insight: NOBODY has dictation + system commands + cross-platform + single binary
- Voxtype is only 3 months old - the space is young and open
- Voice commands is our differentiator
- Sketched cross-platform architecture (SystemControl trait, platform backends)
- Decision: Direct system calls per platform, not MCP server routing
- SS9K could be the first of its kind
- Pipeline: Hotkey ✅ | Audio ✅ | Whisper ✅ | Type ✅ | GPU ✅ | Commands ✅ | Config ✅
- Pushed to GitHub

* Related Work

** Competition Analysis (2026-01-10)

| Tool         | Type      | Platform        | System Control | Setup           | Stars | Status      | Notes                                     |
|--------------+-----------+-----------------+----------------+-----------------+-------+-------------+-------------------------------------------|
| *SS9K*         | Dict+Ctrl | Cross (planned) | Yes            | Binary          | -     | Active      | That's us. First of its kind.             |
| Talon        | Framework | Cross           | Yes            | Python + config | -     | Proprietary | The power-user tool, steep learning curve |
| Voxtype      | Dictation | Linux (Wayland) | No             | Binary          | 144   | Active      | Pure Rust, evdev hotkeys, no voice cmds   |
| OpenWhispr   | Dictation | Cross           | No             | Electron        | 668   | Active      | React+TS+Python, AI routing               |
| Handy        | Dictation | Cross (Tauri)   | No             | Tauri           | 10.5k | Active      | Popular but 58 open issues, unstable      |
| OpenVoiceOS  | Assistant | Embedded        | Skills         | Full OS         | -     | Active      | Mycroft fork, not desktop control         |
| Serenade     | Coding    | Cross           | IDE only       | Install         | 386   | DEAD (2022) | Abandoned July 2022, don't follow         |
| OS built-ins | Dictation | Native          | Varies         | Built-in        | -     | -           | Platform locked, often cloud              |

** Our Differentiator

- *Voice commands for system control* (Voxtype doesn't have this)
- *Single binary* (Talon needs Python, OpenWhispr needs Electron)
- *Cross-platform* (Voxtype is Linux-only)
- *Local/private* (no cloud dependency)
- *Zero setup* (download → run → works)
- *Pure Rust* (no Python bridge like OpenWhispr, no Tauri like Handy)

** What We're Stealing

*** From Voxtype (Linux-first dictation, 144 stars)
- *evdev for Wayland hotkeys*: Kernel-level input works where rdev fails
- *Compositor keybindings*: Native Hyprland/Sway/River integration
- *Output fallback chain*: wtype → ydotool → clipboard (graceful degradation)
- *LLM post-processing hooks*: Pipe transcription through Ollama for cleanup
- *Tokio async runtime*: Already using, validates our approach

*** From Handy (Most popular, 10.5k stars)
- *Silero VAD*: Voice Activity Detection to filter silence before transcription
- *Dual model path*: Whisper (GPU) + Parakeet V3 (CPU-optimized) options
- *Platform-specific keyboard handling*: They learned the hard way (58 issues)
- *What NOT to do*: Their stability issues across platforms = cautionary tale

*** From OpenWhispr (Cross-platform, 668 stars)
- *"Hey [Agent], command" pattern*: Simple trigger word for command mode
- *Multi-provider AI routing*: Clean abstraction over different LLM backends
- *Auto-update with progress bar*: Good UX pattern for model downloads
- *What to beat*: Electron bloat (their binary is huge), Python bridge overhead

*** From Talon (The framework, proprietary)
- *Context-aware commands*: Different command sets per-application
- *Multi-stage intent pipeline*: Priority-based matcher (high/medium/low/fallback)
- *Command vocabulary*: Their wiki is a great reference for naming conventions
- *Community extensibility*: Make customization easy enough for users to share
- *What to avoid*: Steep learning curve, Python dependency, commercial licensing

*** From OpenVoiceOS (Voice assistant framework)
- *Modular skill architecture*: Discrete, composable capability modules
- *Fallback/persona pattern*: LLM handles unmatched intents gracefully
- *Embedded-first mindset*: Design for low-spec hardware = efficient everywhere
- *What's different*: They're Alexa-replacement, we're desktop control

*** From Serenade (DEAD - July 2022)
- *Two-stage VAD*: WebRTC (fast) + Silero (accurate) - good pattern
- *Tree-sitter for code parsing*: If we ever do code-specific commands
- *What to learn*: THEY ABANDONED IT. Don't over-scope, don't ghost users.

** Implementation Priority (Revised 2026-01-10)

Market share reality check:
- Windows: 72%
- macOS: 15%
- Linux X11: 3-4% ← *already works*
- Linux Wayland: 0.5-1% ← deprioritized

1. *Windows support* - 72% of market, crates claim cross-platform already
2. *macOS support* - 15% of market, needs testing + accessibility permissions
3. *Voice Activity Detection* - Silero VAD to filter silence before inference
4. *System control commands* - Our differentiator
5. *Context awareness* - Talon's per-app command sets (future)
6. *Wayland support* - v2.0 maybe, let Voxtype own that niche for now

** Cross-Platform Phase 1: Windows

*** Current Crate Status (Theoretical)

| Crate      | Purpose         | Windows Support | Notes                         |
|------------+-----------------+-----------------+-------------------------------|
| rdev       | Hotkey capture  | ✅ Claims yes   | Uses Win32 raw input hooks    |
| enigo      | Text/key output | ✅ Claims yes   | Uses SendInput API            |
| cpal       | Audio capture   | ✅ Claims yes   | Uses WASAPI                   |
| whisper-rs | Transcription   | ✅ Claims yes   | whisper.cpp is cross-platform |
| rubato     | Resampling      | ✅ Pure Rust    | No platform deps              |

*** What Needs Testing

1. *Does it compile on Windows?*
   - whisper.cpp needs CMake + C++ toolchain
   - May need MSVC or MinGW
   - Vulkan SDK for GPU, or CUDA toolkit

2. *Does rdev actually work?*
   - Global hotkeys on Windows might need admin rights?
   - Different key codes (VK_F12 vs evdev)

3. *Does enigo actually work?*
   - SendInput should work but needs testing
   - Unicode text insertion?

4. *Audio device enumeration*
   - cpal on WASAPI might have different device naming
   - Need to find microphone automatically

*** Windows-Specific Considerations

- *UAC*: May need to run as admin for global hotkeys
- *Defender*: Unknown binary might get flagged
- *Code signing*: Eventually need to sign for distribution
- *Installer*: .exe direct run first, .msi later

*** Testing Strategy

Since sqrew doesn't have Windows:
1. GitHub Actions CI with windows-latest
2. At minimum: verify it compiles
3. Ideal: find a Windows user to test
4. Nuclear: spin up a Windows VM

*** System Control Commands (Windows)

| Command        | Windows API                      | Crate               |
|----------------+----------------------------------+---------------------|
| volume up/down | IAudioEndpointVolume             | windows-sys         |
| mute           | IAudioEndpointVolume::SetMute    | windows-sys         |
| media next     | keybd_event(VK_MEDIA_NEXT_TRACK) | enigo / windows-sys |
| media play     | keybd_event(VK_MEDIA_PLAY_PAUSE) | enigo / windows-sys |
| focus window   | FindWindow + SetForegroundWindow | windows-sys         |
| launch app     | ShellExecuteW                    | windows-sys         |
| close window   | PostMessage(WM_CLOSE)            | windows-sys         |
| screenshot     | BitBlt from desktop DC           | windows-sys         |
| lock screen    | LockWorkStation                  | windows-sys         |

Most of this is just Win32 API calls via windows-sys crate.

** Cross-Platform Phase 2: macOS

*** Crate Status

Same story - rdev, enigo, cpal all claim macOS support.

*** macOS-Specific Considerations

- *Accessibility permissions*: MUST prompt user, no way around it
- *Gatekeeper*: Unsigned binary will show scary warning
- *Code signing*: $99/year Apple Developer account
- *Notarization*: Required for distribution without warnings
- *Metal backend*: whisper-rs has Metal feature for Apple Silicon

*** System Control Commands (macOS)

| Command        | macOS API                     | Notes                        |
|----------------+-------------------------------+------------------------------|
| volume up/down | AppleScript or CoreAudio      | AppleScript easier           |
| media next     | MediaPlayer framework / keys  | Fn+F7/F8/F9 or API           |
| focus window   | Accessibility API             | Needs permission             |
| launch app     | NSWorkspace / open command    | Easy                         |
| screenshot     | screencapture CLI or CGImage  | Built-in CLI is easiest      |
| lock screen    | CGSession / pmset             | Multiple options             |

AppleScript is the escape hatch - can do almost anything.

** Cross-Platform Phase 3: Wayland (v2.0+)

Deprioritized. Voxtype owns this niche. When we get there:
- evdev crate for kernel-level hotkeys
- wtype / ydotool for text output (shell out)
- wlr-protocols for compositor integration
- Test on Sway/Hyprland VMs

** Resources

- rmcp-presence: We already built all the Linux system control logic
- Talon wiki: https://talon.wiki/ (good command vocabulary reference)
- Whisper models: https://huggingface.co/ggerganov/whisper.cpp
- Voxtype source: https://github.com/peteonrails/voxtype (evdev patterns)
- Handy wiki: https://github.com/cjpais/Handy/wiki (VAD docs)
- Silero VAD: https://github.com/snakers4/silero-vad
